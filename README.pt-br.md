[üá∫üá∏ View in English](./README.md)
---
# Pipeline de Dados em Tempo Real da Frota de √înibus

![Airflow](https://img.shields.io/badge/Airflow-017CEE?style=for-the-badge&logo=Apache-Airflow&logoColor=white)
![Spark](https://img.shields.io/badge/Spark-E25A1C?style=for-the-badge&logo=Apache-Spark&logoColor=white)
![Kafka](https://img.shields.io/badge/Kafka-231F20?style=for-the-badge&logo=Apache-Kafka&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-2496ED?style=for-the-badge&logo=docker&logoColor=white)
![Power BI](https://img.shields.io/badge/Power%20BI-F2C811?style=for-the-badge&logo=powerbi&logoColor=black)
![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)

---
![Dashboard Screenshot](img/demo.png)

![Dashboard Screenshot](img/demo1.png)

Este projeto implementa um pipeline de engenharia de dados completo (ponta a ponta) que captura dados de transporte p√∫blico de S√£o Paulo em tempo real, processa com Spark, armazena em um data lake local e orquestra todo o fluxo com Airflow.

## üèõÔ∏è Arquitetura

O pipeline foi projetado para ser uma plataforma escal√°vel e robusta para lidar com fluxos de dados em tempo real. Os dados passam por diversas etapas, desde a ingest√£o at√© o armazenamento e a visualiza√ß√£o.

**Fluxo de Dados:**
1.  **Ingest√£o:** Um script Python (`producer.py`) consome a API oficial [Olho Vivo da SPTrans](https://www.sptrans.com.br/desenvolvedores/) para buscar dados de localiza√ß√£o dos √¥nibus em tempo real.
2.  **Fila de Mensagens:** Os dados coletados s√£o publicados como mensagens em um t√≥pico Kafka, criando um fluxo de dados dur√°vel e escal√°vel.
3.  **Processamento de Stream:** Um job Spark Structured Streaming consome os dados do t√≥pico Kafka em tempo real. Ele aplica transforma√ß√µes, limpa os dados e converte os tipos de dados.
4.  **Armazenamento em Data Lake:** Os dados processados s√£o escritos em um Data Lake local no formato colunar de alta efici√™ncia Parquet.
5.  **Orquestra√ß√£o:** Todo o workflow √© agendado, monitorado e automatizado pelo Apache Airflow, garantindo que as tarefas rodem na ordem correta e tratando falhas de forma elegante.
6.  **BI & An√°lise:** Os dados finais armazenados nos arquivos Parquet s√£o conectados ao Power BI para a cria√ß√£o de dashboards interativos e a Jupyter Notebooks para an√°lises aprofundadas com Spark SQL.

## üõ†Ô∏è Tecnologias Utilizadas

- **Ingest√£o de Dados:** Python (`requests`), Apache Kafka
- **Processamento de Dados:** Apache Spark (PySpark)
- **Armazenamento de Dados:** Parquet (Data Lake)
- **Orquestra√ß√£o:** Apache Airflow
- **Containeriza√ß√£o:** Docker & Docker Compose
- **BI & Visualiza√ß√£o:** Power BI
- **An√°lise:** Jupyter Notebook, Spark SQL
- **Ambiente:** Python 3.11, Java 17

## üöÄ Configura√ß√£o e Execu√ß√£o

Siga os passos abaixo para executar o pipeline completo na sua m√°quina local.

### Pr√©-requisitos
- Docker & Docker Compose
- Python 3.11
- Um Token de API do [Olho Vivo da SPTrans](https://www.sptrans.com.br/desenvolvedores/).

### Passos
1.  **Clone o reposit√≥rio:**
    ```bash
    git clone [https://github.com/](https://github.com/)[SEU_USUARIO]/[SEU_REPOSITORIO].git
    cd [SEU_REPOSITORIO]
    ```

2.  **Crie e ative o ambiente virtual:**
    ```bash
    python3.11 -m venv .venv
    source .venv/bin/activate
    ```

3.  **Instale as depend√™ncias Python:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Configure o Token da API:**
    *(Este token √© necess√°rio para rodar o produtor em tempo real ou o coletor de amostras)*.
    ```bash
    export SPTRANS_API_TOKEN='SEU_TOKEN_AQUI'
    ```
    
5.  **Inicie a Infraestrutura:**
    Este comando ir√° iniciar os cont√™ineres do Kafka e Zookeeper em segundo plano.
    ```bash
    docker-compose up -d
    ```

6.  **Inicialize e Execute o Airflow:**
    * **Defina o `AIRFLOW_HOME`:**
        ```bash
        export AIRFLOW_HOME=$(pwd)/airflow
        ```
    * **Inicialize o Banco de Dados:**
        ```bash
        airflow db init
        ```
    * **Crie um Usu√°rio Administrador:**
        ```bash
        airflow users create \
            --username admin \
            --firstname SeuNome \
            --lastname SeuSobrenome \
            --role Admin \
            --email seu.email@example.com \
            --password suasenha
        ```
    * **Inicie o Webserver e o Scheduler (em dois terminais separados):**
        ```bash
        # Terminal 1
        airflow webserver
        
        # Terminal 2
        airflow scheduler
        ```

7.  **Execute o Pipeline:**
    * Acesse a interface do Airflow em `http://localhost:8080`.
    * Fa√ßa login com as credenciais que voc√™ acabou de criar.
    * Encontre a DAG `sptrans_data_pipeline`, ative-a no bot√£o de toggle e a acione usando o bot√£o de play.

## üìä Dashboard & An√°lise

O resultado final deste pipeline √© um data lake de arquivos Parquet, que pode ser usado para an√°lise.

### Dashboard no Power BI
Um relat√≥rio no Power BI foi criado para visualizar os dados coletados, mostrando KPIs, a localiza√ß√£o dos √¥nibus em um mapa e tend√™ncias de atividade ao longo do tempo.

![Dashboard Screenshot](img/demo.png)

### An√°lise com SQL
Os dados tamb√©m podem ser consultados interativamente usando Spark SQL em um Jupyter Notebook, localizado na pasta `/notebooks`.

## üîß Desafios e Aprendizados

A constru√ß√£o deste pipeline ponta a ponta envolveu a supera√ß√£o de diversos desafios reais de engenharia de dados:

-   **Inferno do Ambiente:** O projeto come√ßou com grandes problemas de compatibilidade entre o Spark 4.0, seus conectores e a vers√£o do Java. Isso foi resolvido com o downgrade para as vers√µes est√°veis do Spark 3.5.1 e Python 3.11, criando um ambiente limpo e reprodut√≠vel.
-   **Conflitos de Depend√™ncia:** Os conectores para servi√ßos na nuvem como BigQuery e Delta Lake apresentaram conflitos profundos de depend√™ncia (`NoSuchMethodError`, `ClassNotFoundException`). Isso levou a uma decis√£o estrat√©gica de pivotar para uma solu√ß√£o mais robusta e com depend√™ncias isoladas (salvar em Parquet nativo).
-   **Problemas de Rede/Firewall:** O comando `spark-submit --packages` falhou consistentemente em baixar depend√™ncias devido a um problema de rede local, refor√ßando a decis√£o de usar uma arquitetura que minimizasse downloads externos durante a execu√ß√£o.
-   **Detalhes de Orquestra√ß√£o:** A implementa√ß√£o no Airflow revelou problemas cl√°ssicos de orquestra√ß√£o, como condi√ß√µes de corrida (o consumidor Spark iniciando antes do t√≥pico Kafka ser criado) e o tratamento de c√≥digos de sa√≠da de comandos shell (`gtimeout` no macOS). Estes foram resolvidos com a adi√ß√£o de tarefas de espera e l√≥gica de tratamento de erro na DAG.

## Melhorias Futuras

-   Fazer o deploy de toda a stack para um provedor de nuvem (GCP ou AWS).
-   Utilizar um servi√ßo gerenciado de Kafka (como o Confluent Cloud) em vez de um cont√™iner local.
-   Substituir o data lake Parquet por um Data Warehouse gerenciado como BigQuery ou Snowflake assim que os problemas de rede forem resolvidos.
-   Criar dashboards de BI mais complexos e alertas automatizados.

## üë§ Autor

**Vinicius Moreira**

-   [LinkedIn](https://www.linkedin.com/in/vinicius-moreira-806105350)
-   [GitHub](https://github.com/Viniciuss-Moreira)
